{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "574cfde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b61625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkit: Package 'punkit' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "nltk.download('punkit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341adaa",
   "metadata": {},
   "source": [
    "## Sentence and word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1785b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"\n",
    "Topic sentences are similar to mini thesis statements. Like a thesis statement, a topic sentence has a specific main point. Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph. Like the thesis statement, a topic sentence has a unifying function. But a thesis statement or topic sentence alone doesn’t guarantee unity. An essay is unified if all the paragraphs relate to the thesis, whereas a paragraph is unified if all the sentences relate to the topic sentence. Note: Not all paragraphs need topic sentences. In particular, opening and closing paragraphs, which serve different functions from body paragraphs, generally don’t have topic sentences.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f614255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenization\n",
    "sent = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8d01ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nTopic sentences are similar to mini thesis statements.',\n",
       " 'Like a thesis statement, a topic sentence has a specific main point.',\n",
       " 'Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.',\n",
       " 'Like the thesis statement, a topic sentence has a unifying function.',\n",
       " 'But a thesis statement or topic sentence alone doesn’t guarantee unity.',\n",
       " 'An essay is unified if all the paragraphs relate to the thesis, whereas a paragraph is unified if all the sentences relate to the topic sentence.',\n",
       " 'Note: Not all paragraphs need topic sentences.',\n",
       " 'In particular, opening and closing paragraphs, which serve different functions from body paragraphs, generally don’t have topic sentences.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e1d2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokenisation\n",
    "word = nltk.word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e74c0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic',\n",
       " 'sentences',\n",
       " 'are',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'mini',\n",
       " 'thesis',\n",
       " 'statements',\n",
       " '.',\n",
       " 'Like',\n",
       " 'a',\n",
       " 'thesis',\n",
       " 'statement',\n",
       " ',',\n",
       " 'a',\n",
       " 'topic',\n",
       " 'sentence',\n",
       " 'has',\n",
       " 'a',\n",
       " 'specific',\n",
       " 'main',\n",
       " 'point',\n",
       " '.',\n",
       " 'Whereas',\n",
       " 'the',\n",
       " 'thesis',\n",
       " 'is',\n",
       " 'the',\n",
       " 'main',\n",
       " 'point',\n",
       " 'of',\n",
       " 'the',\n",
       " 'essay',\n",
       " ',',\n",
       " 'the',\n",
       " 'topic',\n",
       " 'sentence',\n",
       " 'is',\n",
       " 'the',\n",
       " 'main',\n",
       " 'point',\n",
       " 'of',\n",
       " 'the',\n",
       " 'paragraph',\n",
       " '.',\n",
       " 'Like',\n",
       " 'the',\n",
       " 'thesis',\n",
       " 'statement',\n",
       " ',',\n",
       " 'a',\n",
       " 'topic',\n",
       " 'sentence',\n",
       " 'has',\n",
       " 'a',\n",
       " 'unifying',\n",
       " 'function',\n",
       " '.',\n",
       " 'But',\n",
       " 'a',\n",
       " 'thesis',\n",
       " 'statement',\n",
       " 'or',\n",
       " 'topic',\n",
       " 'sentence',\n",
       " 'alone',\n",
       " 'doesn',\n",
       " '’',\n",
       " 't',\n",
       " 'guarantee',\n",
       " 'unity',\n",
       " '.',\n",
       " 'An',\n",
       " 'essay',\n",
       " 'is',\n",
       " 'unified',\n",
       " 'if',\n",
       " 'all',\n",
       " 'the',\n",
       " 'paragraphs',\n",
       " 'relate',\n",
       " 'to',\n",
       " 'the',\n",
       " 'thesis',\n",
       " ',',\n",
       " 'whereas',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " 'is',\n",
       " 'unified',\n",
       " 'if',\n",
       " 'all',\n",
       " 'the',\n",
       " 'sentences',\n",
       " 'relate',\n",
       " 'to',\n",
       " 'the',\n",
       " 'topic',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'Note',\n",
       " ':',\n",
       " 'Not',\n",
       " 'all',\n",
       " 'paragraphs',\n",
       " 'need',\n",
       " 'topic',\n",
       " 'sentences',\n",
       " '.',\n",
       " 'In',\n",
       " 'particular',\n",
       " ',',\n",
       " 'opening',\n",
       " 'and',\n",
       " 'closing',\n",
       " 'paragraphs',\n",
       " ',',\n",
       " 'which',\n",
       " 'serve',\n",
       " 'different',\n",
       " 'functions',\n",
       " 'from',\n",
       " 'body',\n",
       " 'paragraphs',\n",
       " ',',\n",
       " 'generally',\n",
       " 'don',\n",
       " '’',\n",
       " 't',\n",
       " 'have',\n",
       " 'topic',\n",
       " 'sentences',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word\n",
    "#even the fullstops are considered as words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26942e3",
   "metadata": {},
   "source": [
    "## Stemming and lemmatization\n",
    "__Process of reducing the infected words to their word stem or to the root form of the word__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da99c526",
   "metadata": {},
   "source": [
    "__Stemming__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "523115d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "962f9e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get all stopwords in the english language\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ea5c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d42e47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sent)):\n",
    "    words = nltk.word_tokenize(sent[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sent[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ace9f718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inparticular',\n",
       " ',',\n",
       " 'opencloseparagraph',\n",
       " ',',\n",
       " 'servdifferfunctionbodiparagraph',\n",
       " ',',\n",
       " 'gener',\n",
       " '’',\n",
       " 'topicsentenc',\n",
       " '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b70735ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic sentenc similar mini thesi statement .',\n",
       " 'like thesi statement , topic sentenc specif main point .',\n",
       " 'wherea thesi main point essay , topic sentenc main point paragraph .',\n",
       " 'like thesi statement , topic sentenc unifi function .',\n",
       " 'but thesi statement topic sentenc alon ’ guarante uniti .',\n",
       " 'an essay unifi paragraph relat thesi , wherea paragraph unifi sentenc relat topic sentenc .',\n",
       " 'note : not paragraph need topic sentenc .',\n",
       " 'in particular , open close paragraph , serv differ function bodi paragraph , gener ’ topic sentenc .']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can see that lemmatization not always ives the accurate stem word\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c1b27",
   "metadata": {},
   "source": [
    "__Lemmatization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1c2be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23ca8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01946c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d8fe792",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sent)):\n",
    "    words = nltk.word_tokenize(sent[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sent[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2495d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nTopic sentences are similar to mini thesis statements.',\n",
       " 'Like a thesis statement, a topic sentence has a specific main point.',\n",
       " 'Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.',\n",
       " 'Like the thesis statement, a topic sentence has a unifying function.',\n",
       " 'But a thesis statement or topic sentence alone doesn’t guarantee unity.',\n",
       " 'An essay is unified if all the paragraphs relate to the thesis, whereas a paragraph is unified if all the sentences relate to the topic sentence.',\n",
       " 'Note: Not all paragraphs need topic sentences.',\n",
       " 'In particular, opening and closing paragraphs, which serve different functions from body paragraphs, generally don’t have topic sentences.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe136d43",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1371dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b4031c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nTopic sentences are similar to mini thesis statements.',\n",
       " 'Like a thesis statement, a topic sentence has a specific main point.',\n",
       " 'Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.',\n",
       " 'Like the thesis statement, a topic sentence has a unifying function.',\n",
       " 'But a thesis statement or topic sentence alone doesn’t guarantee unity.',\n",
       " 'An essay is unified if all the paragraphs relate to the thesis, whereas a paragraph is unified if all the sentences relate to the topic sentence.',\n",
       " 'Note: Not all paragraphs need topic sentences.',\n",
       " 'In particular, opening and closing paragraphs, which serve different functions from body paragraphs, generally don’t have topic sentences.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "37020fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(sent)):\n",
    "    review = re.sub('[^a-zA-Z0-9 ]', '', sent[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatizer.lemmatize(words) for words in review if words not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ba8f700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic sentence similar mini thesis statement',\n",
       " 'like thesis statement topic sentence specific main point',\n",
       " 'whereas thesis main point essay topic sentence main point paragraph',\n",
       " 'like thesis statement topic sentence unifying function',\n",
       " 'thesis statement topic sentence alone doesnt guarantee unity',\n",
       " 'essay unified paragraph relate thesis whereas paragraph unified sentence relate topic sentence',\n",
       " 'note paragraph need topic sentence',\n",
       " 'particular opening closing paragraph serve different function body paragraph generally dont topic sentence']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8addeee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nTopic sentences are similar to mini thesis statements.',\n",
       " 'Like a thesis statement, a topic sentence has a specific main point.',\n",
       " 'Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.',\n",
       " 'Like the thesis statement, a topic sentence has a unifying function.',\n",
       " 'But a thesis statement or topic sentence alone doesn’t guarantee unity.',\n",
       " 'An essay is unified if all the paragraphs relate to the thesis, whereas a paragraph is unified if all the sentences relate to the topic sentence.',\n",
       " 'Note: Not all paragraphs need topic sentences.',\n",
       " 'In particular, opening and closing paragraphs, which serve different functions from body paragraphs, generally don’t have topic sentences.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ca4606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2a22696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 31)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb333e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "747ebbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 1, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0,\n",
       "        0, 0, 0, 1, 1, 2, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d006fd6",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "__Unlike the BoW model, it brings out semantic meaning from the words__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "40dec35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab4f34c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.58018778, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.23169722, 0.        , 0.58018778, 0.        , 0.36788576,\n",
       "        0.28992608, 0.23169722, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.41501972, 0.41501972, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.41501972, 0.        ,\n",
       "        0.19775912, 0.        , 0.        , 0.49520414, 0.31399929,\n",
       "        0.24745884, 0.19775912, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.29640769, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.59281538, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.22425876, 0.        , 0.59281538, 0.        ,\n",
       "        0.14123985, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.17673547, 0.14123985, 0.        , 0.        , 0.        ,\n",
       "        0.29640769],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.45615965, 0.        , 0.        ,\n",
       "        0.45615965, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.21736252, 0.        , 0.        , 0.        , 0.34512531,\n",
       "        0.27198886, 0.21736252, 0.        , 0.54429257, 0.        ,\n",
       "        0.        ],\n",
       "       [0.44852847, 0.        , 0.        , 0.        , 0.44852847,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.44852847,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.17911925, 0.        , 0.        , 0.        , 0.28440316,\n",
       "        0.2241345 , 0.17911925, 0.        , 0.        , 0.44852847,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24132882, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.3651734 , 0.        , 0.        , 0.57591014,\n",
       "        0.22998895, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.14389425, 0.11499448, 0.57591014, 0.        , 0.        ,\n",
       "        0.24132882],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.60622576, 0.60622576,\n",
       "        0.        , 0.38439594, 0.        , 0.        , 0.        ,\n",
       "        0.24209546, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24209546, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.3067201 , 0.3067201 , 0.3067201 , 0.        ,\n",
       "        0.3067201 , 0.        , 0.25705538, 0.3067201 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.3067201 , 0.38897047, 0.3067201 , 0.        , 0.        ,\n",
       "        0.12248827, 0.3067201 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.12248827, 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c44e2",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1bab262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "62767b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim converts a word into a vector of 100 dimensions\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0dedc315",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2688f21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nTopic sentences are similar to mini thesis statements.',\n",
       " 'Like a thesis statement, a topic sentence has a specific main point.',\n",
       " 'Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.',\n",
       " 'Like the thesis statement, a topic sentence has a unifying function.',\n",
       " 'But a thesis statement or topic sentence alone doesn’t guarantee unity.',\n",
       " 'An essay is unified if all the paragraphs relate to the thesis, whereas a paragraph is unified if all the sentences relate to the topic sentence.',\n",
       " 'Note: Not all paragraphs need topic sentences.',\n",
       " 'In particular, opening and closing paragraphs, which serve different functions from body paragraphs, generally don’t have topic sentences.']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "362f2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = []\n",
    "for i in range(len(sent)):\n",
    "    review = re.sub('[^a-zA-Z0-9 ]', '', sent[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatizer.lemmatize(words) for words in review if words not in set(stopwords.words('english'))]\n",
    "    review = [item for item in review]\n",
    "    corpus1.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1c00a8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['topic', 'sentence', 'similar', 'mini', 'thesis', 'statement'],\n",
       " ['like',\n",
       "  'thesis',\n",
       "  'statement',\n",
       "  'topic',\n",
       "  'sentence',\n",
       "  'specific',\n",
       "  'main',\n",
       "  'point'],\n",
       " ['whereas',\n",
       "  'thesis',\n",
       "  'main',\n",
       "  'point',\n",
       "  'essay',\n",
       "  'topic',\n",
       "  'sentence',\n",
       "  'main',\n",
       "  'point',\n",
       "  'paragraph'],\n",
       " ['like', 'thesis', 'statement', 'topic', 'sentence', 'unifying', 'function'],\n",
       " ['thesis',\n",
       "  'statement',\n",
       "  'topic',\n",
       "  'sentence',\n",
       "  'alone',\n",
       "  'doesnt',\n",
       "  'guarantee',\n",
       "  'unity'],\n",
       " ['essay',\n",
       "  'unified',\n",
       "  'paragraph',\n",
       "  'relate',\n",
       "  'thesis',\n",
       "  'whereas',\n",
       "  'paragraph',\n",
       "  'unified',\n",
       "  'sentence',\n",
       "  'relate',\n",
       "  'topic',\n",
       "  'sentence'],\n",
       " ['note', 'paragraph', 'need', 'topic', 'sentence'],\n",
       " ['particular',\n",
       "  'opening',\n",
       "  'closing',\n",
       "  'paragraph',\n",
       "  'serve',\n",
       "  'different',\n",
       "  'function',\n",
       "  'body',\n",
       "  'paragraph',\n",
       "  'generally',\n",
       "  'dont',\n",
       "  'topic',\n",
       "  'sentence']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "15a1503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the word to vec model\n",
    "model = Word2Vec(corpus1, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6af689d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the 100 vectors for 'paragraph' word \n",
    "vector = model.wv['paragraph']\n",
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "41bb7459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('essay', 0.19910889863967896),\n",
       " ('body', 0.17554187774658203),\n",
       " ('relate', 0.17317116260528564),\n",
       " ('statement', 0.17129895091056824),\n",
       " ('generally', 0.15394803881645203),\n",
       " ('guarantee', 0.14873789250850677),\n",
       " ('main', 0.1474504917860031),\n",
       " ('point', 0.06394331157207489),\n",
       " ('serve', 0.059571411460638046),\n",
       " ('closing', 0.054926663637161255)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the similar word for 'paragraph'\n",
    "similar = model.wv.most_similar('paragraph')\n",
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f75b9b6",
   "metadata": {},
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "868af9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nTopic sentences are similar to mini thesis statements.',\n",
       " 'Like a thesis statement, a topic sentence has a specific main point.',\n",
       " 'Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.',\n",
       " 'Like the thesis statement, a topic sentence has a unifying function.',\n",
       " 'But a thesis statement or topic sentence alone doesn’t guarantee unity.',\n",
       " 'An essay is unified if all the paragraphs relate to the thesis, whereas a paragraph is unified if all the sentences relate to the topic sentence.',\n",
       " 'Note: Not all paragraphs need topic sentences.',\n",
       " 'In particular, opening and closing paragraphs, which serve different functions from body paragraphs, generally don’t have topic sentences.']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = nltk.sent_tokenize(para)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0734c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_N_grams(text,ngram):\n",
    "    review = re.sub('[^a-zA-Z0-9 ]', '', text)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatizer.lemmatize(words) for words in review if words not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    words=[word for word in review.split(\" \") if word not in set(stopwords.words('english'))]  \n",
    "    print(\"Sentence after removing stopwords:\",words)\n",
    "    temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "    ans=[' '.join(ngram) for ngram in temp]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de94801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after removing stopwords: ['like', 'thesis', 'statement', 'topic', 'sentence', 'specific', 'main', 'point']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'thesis',\n",
       " 'statement',\n",
       " 'topic',\n",
       " 'sentence',\n",
       " 'specific',\n",
       " 'main',\n",
       " 'point']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_N_grams('Like a thesis statement, a topic sentence has a specific main point.',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595a5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
